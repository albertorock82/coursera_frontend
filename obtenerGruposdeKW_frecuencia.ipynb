{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2DOu4U99E4rNZGrXVWYKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertorock82/coursera_frontend/blob/main/obtenerGruposdeKW_frecuencia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gspread pandas unidecode nltk\n"
      ],
      "metadata": {
        "id": "8ZrDGB1KWYrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDCDFvP7UBep",
        "outputId": "8e6fbc63-8656-48b2-b1c4-5ede508e3ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Palabras agrupadas y guardadas en la hoja 'grupos'\n"
          ]
        }
      ],
      "source": [
        "# Paso 1: Instalar dependencias\n",
        "!pip install --quiet gspread pandas unidecode nltk\n",
        "\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Descargar stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from google.colab import auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "# --- PARAMETROS (MODIFICA ESTO) ---\n",
        "NOMBRE_ARCHIVO_JSON = '/content/ambient-antenna-440101-u4-d9758bb09208.json'  # Sube tu clave JSON y pon aquí el nombre\n",
        "NOMBRE_LIBRO = 'KeywordDepliadorKS_serp'\n",
        "NOMBRE_HOJA = 'Sheet1'  # hoja donde están las keywords en columna A\n",
        "NOMBRE_HOJA_RESULTADO = 'grupos'\n",
        "\n",
        "# --- AUTENTICACIÓN ---\n",
        "scope = [\"https://spreadsheets.google.com/feeds\",'https://www.googleapis.com/auth/spreadsheets',\n",
        "         \"https://www.googleapis.com/auth/drive.file\",\"https://www.googleapis.com/auth/drive\"]\n",
        "\n",
        "credentials = Credentials.from_service_account_file(NOMBRE_ARCHIVO_JSON, scopes=scope)\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "# --- CARGAR DATOS DEL SHEET ---\n",
        "sh = gc.open(NOMBRE_LIBRO)\n",
        "worksheet = sh.worksheet(NOMBRE_HOJA)\n",
        "keywords = worksheet.col_values(1)  # columna A\n",
        "\n",
        "# --- LIMPIEZA Y TOKENIZACIÓN ---\n",
        "# Stopwords en español + extras\n",
        "stopwords_es = set(stopwords.words('spanish'))\n",
        "stopwords_extras = set([\"como\", \"es\", \"de\", \"los\", \"las\", \"para\", \"el\", \"la\", \"por\", \"con\", \"un\", \"una\", \"al\", \"en\"])\n",
        "stopwords_all = stopwords_es.union(stopwords_extras)\n",
        "\n",
        "# Expresión regular para detectar palabras que tienen solo números\n",
        "regex_solo_numeros = re.compile(r'^\\d+$')\n",
        "\n",
        "# Preprocesar y tokenizar\n",
        "def limpiar_y_tokenizar(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = unidecode(texto)  # elimina tildes\n",
        "    palabras = texto.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "    palabras_filtradas = []\n",
        "    for p in palabras:\n",
        "        if (\n",
        "            len(p) >= 3 and                           # Al menos 3 letras\n",
        "            p not in stopwords_all and               # No esté en stopwords\n",
        "            not re.match(r'^\\d+$', p)                # No sea solo números\n",
        "        ):\n",
        "            palabras_filtradas.append(p)\n",
        "\n",
        "    return palabras_filtradas\n",
        "\n",
        "\n",
        "# Contar palabras\n",
        "from collections import Counter\n",
        "contador = Counter()\n",
        "\n",
        "for kw in keywords:\n",
        "    palabras = limpiar_y_tokenizar(kw)\n",
        "    contador.update(palabras)\n",
        "\n",
        "# Convertir resultados en DataFrame ordenado\n",
        "df_resultado = pd.DataFrame(contador.items(), columns=[\"palabra\", \"frecuencia\"]).sort_values(\"frecuencia\", ascending=False)\n",
        "\n",
        "# --- CREAR NUEVA HOJA Y CARGAR RESULTADOS ---\n",
        "# Si la hoja \"grupos\" ya existe, borrarla\n",
        "try:\n",
        "    sh.del_worksheet(sh.worksheet(NOMBRE_HOJA_RESULTADO))\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Crear nueva hoja y cargar\n",
        "worksheet_nueva = sh.add_worksheet(title=NOMBRE_HOJA_RESULTADO, rows=len(df_resultado)+1, cols=2)\n",
        "worksheet_nueva.update([[\"palabra\", \"frecuencia\"]] + df_resultado.values.tolist())\n",
        "\n",
        "print(\"✅ Palabras agrupadas y guardadas en la hoja 'grupos'\")\n"
      ]
    }
  ]
}